# Пособие о конкурентноти в Python

С развитием приложений, практически всегда наступает момент когда производительности простых, поочередно выполняющихся частей программы становится недостаточно.
Для повышения производительности приложения есть несколько методов:
* Повысить производительность железа
* Использовать для написания программы более быстрые и как правило низкоуровневые языки программирования.
* Применять для решения задач более эффективные алгоритмы
* Одновременно выполнять несколько задач

Про первые три пункта я здесь рассказывать не стану, а остановлюсь только на последнем.
Есть в Python такие чудесные библиотеки:
* asyncio
* multithreading
* multiprocessing

Все они позводяют нам выполнить одновременно несколько задач, но вот подходы координально различаются.

Начнем с определений, когда мы хотим выполнять одновременно более одной задачи, 
нам нужно разобраться с двумя понятиями: конкурентность и параллельность.

Конкурентность - возможность одновременного запуска нескольких программ или нескольких частей программы.
Программы в данном случае вовсе не обязательно будут выполняться одновременно

Параллелизм - одновременное выполнение нескольких задач.

Для того, чтобы описать разницу между этими понятиями, воспользуюсь примером Роба Пайка из его доклада "Concurrency is Not Parallelism". 
Представим задачу: у нас есть большая куча книг, есть печь в которой нужно эти книги сжечь и есть суслик, которому мы и поручим эту работу.

![img_2.png](images/one_gopher.png)

Задача конечно выполняется, книг становится всё меньше, но вот один суслик работает очень медленно.
Дадим ему в помощь ещё несколько сусликов.

![img.png](images/many_gophers_concurent.png)

Вот, другое дело! Работа закипела! 
Но есть ньюанс, сусликов стало в 4 раза больше, но вот их сумарная производительность возрасла немного меньше чем в 4 раза.
Почему так вышло? Теперь суслики помимо основной работы по сжиганию книг, должны внимательно следить друг за другом, чтобы банально не столкнуться, 
а также обмениваться сообщениями по типу "Я закончил сжигать книги и пошел за другими, ты можешь начать сжигать свои".
А ещё, если суслик подошел к печи с книгами, а там в это время работает другой суслик, тогда одному из них придется просто стоять и ждать без дела. 
Всё это накладывает расходы на систему, занижая общую производительность.

Кстати в зависимости от архитектуры, все суслики могут выполнять полный цикл: 
брать книги из кучи, вести их к печи, сжигать и возвращаться с пустой тележкой за следующей партией книг.
А могут отвечать за разные задачи: один суслик загружает книги в тележку, второй везет её к печи и возвращается обратно за следующей тележкой, 
третий сжигает книги в печи, а четвертый возвращает пустую тележку первому.
Свои недостатки есть как у первого так и у второго варианта, поэтому однозначного ответа для всех задач нет, нужно смотреть по ситуации.

Но есть и другой подход. Возьмем трех сусликов и дадим каждому из них свою стопку книг, свою тележку и свою печь.
Теперь они будут работать полностью независимо друг от друга. Вот это и называется параллельностью!

![img.png](images/many_gopher_parallel.png)


# Немного истории

Давайте вкратце рассмотрим, какими были компьютеры раньше. 
Первые компьютеры и первые операционные системы были очень простыми, 
они имели всего одно ядро и выполняли всего лишь одну задачу за раз.
Примером однозадачной операционной системы является MS-DOS, которая была очень популярной системой с 1980-х до середины 1990-х годов.
Система могла выполнять лишь одну программу, поэтому для запуска другой программы, первую приходилось выгрузить из памяти и запустить вместо неё новую.

Заставить кусок железа выполнять программы это конечно хорошо, но лучше бы чтобы он мог выполнять сразу несколько программ.
Тогда программисты придумали абстракцию - процесс. Все программы выгружались в память, каждая в отдельном процессе и выполнялись по очереди, передавая управление друг другу.
Первая программа немного поработала, передала управление операционной системе, она в свою очередь дала поработать второй, вторая вернула управление и оно перешко к третьей, далее опять первая и так на протяжении всей работы компьютера.
Однако быстро обнаружились проблемы - не все программы идеальны, скорее даже никакие:), и вот если одна из программ зависала, она не могла передать управление системе.
Что же в результате? Все запущенные программы во главе с операционной системой зависли и карета превратилась в тыкву. 
В добавок к этому, у всех программ была общая память, программы буквально могли залесть в ячейки которые используются другой программой и попортить ей жизнь (нарочно или же по ошибке программиста). 
Такой подход именовали **кооперативной (невытесняющей) многозадачностью**.

Разумеется такие проблемы серьезно мешают пользователям, мало кто хочет перезагружать компьютер после каждого бага в программе.
Тогда был разработан другой подход - **вытесняющая многозадачность**. 
Опять одно ядро, программы и несколько процессов, но только вот они больше не решают когда им отдать управление управление, теперь всем рулит операционная система.
Принцип такой: процесс работает некоторое время, затем процессор усыпляет программу и будит операционную систему, она смотрит все спящие процессы и решает кому из них просыпаться следующим.
Зависший процесс больше не останавливает всю систему. 
Проблема общей памяти также была решена, теперь каждому процессу выделяется своя память, в рамках которой он мог работать.


# Что такое GIL и как он работает

